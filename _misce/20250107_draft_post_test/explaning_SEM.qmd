---
title: "Understanding Sampling Distributions and Standard Error"
author: "Generated by ChatGPT"
date: "`r Sys.Date()`"
format: html
---

```{r setup, include=FALSE}
renv::use("tidyverse", "patchwork")
library(tidyverse)
library(patchwork)
set.seed(123)  # For reproducibility
```

# Introduction

This document explores the **variance of sample means** and the **standard error of the mean (SEM)**, which are fundamental to understanding hypothesis testing, particularly **Z-tests**. We illustrate how the variability of sample means decreases with larger sample sizes and provide visualizations to support these ideas.

# Fundamental Concepts

## Variance of Sample Mean

A key statistical principle states that the variance of the sample mean is:

$$
Var(\bar{X}) = \frac{\sigma^2}{n}
$$

where:

-   $\sigma^2$ is the population variance,
-   $n$ is the sample size.

Since sample means are averaged over multiple observations, their variance is smaller than the population variance, making larger samples more stable.

The **standard error of the mean (SEM)** is the standard deviation of the sampling distribution of the mean:

$$
SEM = \frac{\sigma}{\sqrt{n}}
$$

This measures how much sample means are expected to fluctuate from sample to sample.

# Simulating a Population

We generate a normally distributed population:

```{r}
population <- rnorm(100000, mean = 50, sd = 15)  # Large population with mean 50 and SD 15
```

# The Central Limit Theorem and Sampling Distribution

The **Central Limit Theorem (CLT)** states that the distribution of sample means approaches normality as sample size increases, even if the original population is not normal.

-   If the population is normal, the sample mean distribution is also normal.
-   If the population is not normal, the sample mean distribution becomes approximately normal for larger samples (typically ( n \geq 30 )).

### **Visualizing the Sampling Distribution of the Mean**

We simulate the sampling distribution by repeatedly drawing samples and computing their means:

```{r}
sample_sizes <- c(5, 10, 30, 50, 100)
num_samples <- 1000

sampling_distributions <- map(sample_sizes, ~ tibble(
  mean = replicate(num_samples, mean(sample(population, size = .x, replace = TRUE)))
))

# Define common x-axis limits
x_limits <- range(unlist(map(sampling_distributions, ~ range(.x$mean))))

sampling_distributions %>% 
  map2(sample_sizes, ~ ggplot(.x, aes(mean)) + 
         geom_histogram(bins = 30, fill = "lightblue", color = "black", alpha = 0.7) +
         ggtitle(paste("Sample Size =", .y)) +
         xlab("Sample Mean") +
         xlim(x_limits) +
         theme_minimal()) %>% 
  wrap_plots()
```

This visualization demonstrates how the spread of sample means **decreases** as sample size increases, making the estimates of the population mean more precise. This reduction in variance is exactly why the denominator of the Z-test formula includes ( \sqrt{n} ).

# Standard Error vs Sample Size

To further illustrate why the denominator in the Z-test formula depends on ( n ), we visualize how the standard error decreases as sample size increases:

```{r}
se_values <- map_dbl(sample_sizes, ~ sd(sample(population, size = .x, replace = TRUE)) / sqrt(.x))

tibble(sample_size = sample_sizes, SEM = se_values) %>% 
  ggplot(aes(sample_size, SEM)) + 
  geom_line(color = "red") +
  geom_point(color = "red", size = 3) +
  labs(title = "Standard Error vs Sample Size",
       x = "Sample Size", y = "Standard Error of the Mean (SEM)") +
  theme_minimal()
```

This visualization reinforces the concept that **as sample size increases, the standard error decreases**, which leads to more precise estimates of the population mean and **greater power in hypothesis tests** such as Z-tests.
